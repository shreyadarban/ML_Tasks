# -*- coding: utf-8 -*-
"""Task1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wpZgnHAgblLdrLrrrqCqRSokagFHFek3
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.datasets import load_iris

# Load the dataset
data = pd.read_csv('Iris.csv')

# Check if 'Id' column exists
if 'Id' in data.columns:
    data.drop('Id', axis=1, inplace=True)
else:
    print("'Id' column not found in the dataset.")

# Visualize pairplot
sns.pairplot(data, hue='Species')
plt.show()

# Separate features and target variable
X = data.drop('Species', axis=1)
y = data['Species']

# Split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)

# Define values of k for KNN
kval = range(1, 21)
# Lists to store training and testing accuracy
train_score_list = []
test_score_list = []

# Lists to store training and testing accuracy
train_score_list = []
test_score_list = []

# Train KNN models for different values of k
for k in kval:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(x_train, y_train)
    y_pred_train = knn.predict(x_train)
    y_pred_test = knn.predict(x_test)
    train_score_list.append(accuracy_score(y_train, y_pred_train))
    test_score_list.append(accuracy_score(y_test, y_pred_test))

# Plot the training and testing accuracy for different values of k
plt.plot(kval, train_score_list, color='r', label='training accuracy')
plt.plot(kval, test_score_list, color='b', label='testing accuracy')
plt.xlabel("Value of k for KNN")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

# Select the best k based on testing accuracy
best_k = kval[test_score_list.index(max(test_score_list))]
print("Best k:", best_k)

# Train the final KNN model with the best k
knn = KNeighborsClassifier(n_neighbors=best_k)
knn.fit(x_train, y_train)

# Make predictions on test data
y_pred_test = knn.predict(x_test)

# Calculate accuracy on test data
test_accuracy = accuracy_score(y_test, y_pred_test)
print("Test accuracy:", test_accuracy)

# Predictions for new data points with feature names
new_data_points = [[5.6, 3, 4.6, 1.2], [7.6, 3, 5.6, 2.2], [2.6, 2, 1.6, 1.2]]
predictions = knn.predict(new_data_points)
print("Predictions for new data points:")
for data_point, prediction in zip(new_data_points, predictions):
    print("Prediction for {}: {}".format(data_point, prediction))

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features by removing the mean and scaling to unit variance
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
# Initialize and train the Support Vector Classifier (SVC) model
svc_model = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)
svc_model.fit(X_train_scaled, y_train)

# Make predictions on the testing set using SVC
y_pred_svc = svc_model.predict(X_test_scaled)

# Evaluate the SVC model
accuracy_svc = accuracy_score(y_test, y_pred_svc)
print("Support Vector Classifier (SVC) Model Accuracy:", accuracy_svc)
print("\nClassification Report for SVC:")
print(classification_report(y_test, y_pred_svc, target_names=iris.target_names))

# Plot confusion matrix for SVC
cm_svc = confusion_matrix(y_test, y_pred_svc)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_svc, annot=True, cmap='magma', xticklabels=iris.target_names, yticklabels=iris.target_names)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix for SVC')
plt.show()